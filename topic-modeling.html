---
layout: default
permalink: topic-modeling
---
<head>
  <style>
    h4 {text-align: center;}
  </style>
</head>

<body>
  <h4>
    <a href="https://pnlpuos.github.io/overview">Overview</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/data-preparation">Data Preparation</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/topic-modeling">Topic Modeling</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/sentiment-analysis">Sentiment Analysis</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/outcome">Outcome and Reflection</a><a>&emsp;</a>
  </h4>
</body>

<h1>Topic Modeling</h1>
<p>
  <h3>Approach</h3>
  As mentioned in the <a href="https://pnlpuos.github.io/overview">Overview</a>
  section of this report, our topic analysis pipeline employs a document
  clustering approach which dynamically-identifies an optimal clustering of
  vectorized document representations, before passing clustered documents onto
  later tasks such as <a href="https://pnlpuos.github.io/sentiment-analysis">Sentiment
  Analysis</a>. However, it must be admitted that this approach is not the
  most-common method for achieving the task of topic modeling, and we below
  provide an overview of the task of topic modeling to contextualize the
  subsequent deep-dive into our methodology. <br>

  <a>&emsp;</a>Topic modeling is the natural language processing (NLP) task of identifying
  representative groups of topics in a text dataset. In other words, when
  provided a set of documents, an effective topic modeling will demonstrate
  'what the documents are about.' Given that we implement a topic modeling
  pipeline for reporting on open employee survey comments, reasonable topics
  identified by a successful modeling may include topics such as <i>salary</i>,
  <i>teamwork</i>, etc. <br>

  <a>&emsp;</a>There exist two main approaches to the topic modeling task: statistical
  topic model approaches and document clustering approaches. We briefly
  describe the statistical approach before investigating our methods.

<p>[SUBIR]</p>

<p>
  <a>&emsp;</a>We employ a document clustering approach given our academic interest in
  exploring the applications of pretrained word embeddings coupled with
  traditional cluster analyses. Furthermore, we ignore supervised approaches
  such as document classification because these are unsuitable for our task
  given the requirement that the pipeline be able to dynamically-identify
  new topics potentially unrepresented in a training set. <br>

  <a>&emsp;</a>The document clustering approach requires that we first transform the
  unstructured comment data into numerical features before
  performing a cluster analysis on the feature-engineered documents. After
  feature-engineering we obtain document vectors for which we perform a
  cluster analysis optimized via hyperparameter grid search, before
  generating meaningful topic labels useful for report generation. The below
  image summarizes the topic modeling pipeline as implemented in our
  codebase.
</p>

<p>
  <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/pipeline-topics.png" alt="...">
</p>
</p>

<p>
  <h3>Document Vectorization</h3>
  While it is a trivial task to obtain word vectors from pretrained neural word embedding models
  such as word2vec or FastText, it is challenging to devise a methodology for converting these
  word-level vectors into a normalized document representation which still encodes the
  semantic information of the document's concatenated tokens. <br>

  <a>&emsp;</a>Many solutions to the problem of document vectorization leverage the
  demonstrated <i>compositionality</i> of obtained word embeddings, i.e. the behavior that
  larger blocks of information such as phrases can be represented by the vector manipulation of
  embedding constituents. [1] Due to this property of trained word embeddings, one can negotiate
  vector operations such as addition to superficially 'combine' the semantic information
  encoded in individual tokens into a document representation. <br>

  <a>&emsp;</a>A naïve solution for obtaining a document vector is the simple average of
  embedding vectors constituting a document. As an initial approach, we employed this method
  for obtaining document vectors, but the resulting clusterings did not yield optimal results.
  The figure below demonstrates a sample clustering on FastText vectors averaged to obtain
  document vectors. Clearly, the clusters are not clearly separable. <br>
</p>
<p>
  <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/graph-badclusters.png" alt="...">
</p>

<p>
  <a>&emsp;</a>It has been shown that weighting embeddings via smoothed term frequency can
  improve performance on text similarity tasks without the need for additional training data. [3]
  Because of these findings, we opted to implement additional functionality for computing the
  smooth-inverse-frequency (SIF) weighted average of word embeddings as a document representation.
  The SIF weighting equation for a single word is demonstrated below: <br>
</p>
<p>
  <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/eq-sif.png" alt="...">
</p>

<p>
  where <i>a</i> denotes an arbitrary normalization constant of approximately 1e-5, and <i>p(w)</i>
  denotes the word's frequency. Nevertheless, preliminary evaluations showed that this baseline,
  although effective for text similarity, did not generalize well to the creation of document
  vectors which performed effectively for document clustering. <br>
  <a>&emsp;</a>Inspired by reserch into effective document representations for twitter sentiment
  analysis, a domain which is somewhat comparable to ours given similarities between twitter
  texts and our short, focused comments, we opted to integrate into the SIF weighted average the
  Term-Frequency-Inverse-Document-Frequency (TFIDF) measure as a replacement for the value <i>p(w)</i>,
  as was found to be itself effective as a weighting schema for document vectors. [4, 5] <br>
  <a>&emsp;</a>With this modification, the weighting equation for single words in our
  averaged document vectors becomes <br>
</p>
<p>
  <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/eq-siftfidf.png" alt="...">
</p>

<p>
  where <i>w</i> for a term <i>i</i> in document <i>j</i> denotes<br>
</p>
<p>
  <img src="https://www.link-assistant.com/images/news/tf-idf-tool-for-seo/screen-03.png" alt="...">
</p>

<p>
  The resulting weighting scheme produces satisfactory clusterings when qualitatively
  compared with other approaches. Seen below, a sample clustering with increased
  separation between identified clusters suggests that the TFIDF weighting scheme captures
  the most-relevant keywords of each document and effectively weights these terms in order
  to generate more meaningfully-distinct document vector representations.
</p>
<p>
  <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/graph-goodclusters.png" alt="...">
</p>
</p>

<p>
  <h3>Dimensionality Reduction</h3>
  ...
<p>
  ...
</p>
</p>

<p>
  <h3>Clustering</h3>
  ...
<p>
  ...
</p>
</p>

<p>
  <h3>Topic Labeling</h3>
  ...
<p>
  ...
</p>
</p>

<p>
  <h3>Grid Search</h3>
  ...
<p>
  ...
</p>
</p>

<p>
  <h3>Analysis of Grid Search Logs</h3>
  ...
<p>
  ...
</p>
</p>

<p>
  <h3>References</h3>
  <a>&emsp;</a>[1] Mikolov, Tomas & Corrado, G.s & Chen, Kai & Dean, Jeffrey. (2013). Efficient Estimation of Word Representations in Vector Space. 1-12.<br>
  <a>&emsp;</a>[2] P. Bojanowski, E. Grave, A. Joulin, T. Mikolov. (2016) Enriching Word Vectors with Subword Information.<br>
  <a>&emsp;</a>[3] Arora, S., Liang, Y., & Ma, T. (2016). A simple but tough-to-beat baseline for sentence embeddings. Paper presented at 5th International Conference on Learning Representations. <br>
  <a>&emsp;</a>[4] Edilson Anselmo Correa Junior, Vanessa Marinho, and Leandro Santos. (2017). NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter Sentiment Analysis. In Proceedings of the 11th International Workshop on Semantic Evaluation. Vancouver, Canada, SemEval ’17, pages 610–614.<br>
  <a>&emsp;</a>[5] Zhao, Jiang & Lan, Man & Tian, Jun. (2015). ECNU: Using Traditional Similarity Measurements and Word Embedding for Semantic Textual Similarity Estimation.<br>
</p>
