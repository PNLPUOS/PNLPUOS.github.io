---
layout: default
permalink: data-preparation
---
<head>
  <style>
    h4 {text-align: center;}
  </style>
</head>

<body>
  <h4>
    <a href="https://pnlpuos.github.io/overview">Overview</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/data-preparation">Data Preparation</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/topic-modeling">Topic Modeling</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/sentiment-analysis">Sentiment Analysis</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/outcome">Outcome and Reflection</a><a>&emsp;</a>
  </h4>
</body>

<h1>Data Preparation</h1>
<p>
  In the modern world everyone is associated with data. We have most access to the data than ever before. If in 1640s a latin word called “data”[1] was not recorded then no one would have known the term. Data is a collection of facts, such as numbers, words, measurements, observations or just descriptions of things. Nowadays, data scientists are always collecting data from different sources like social media, news portal, forum etc and extracting information. These massive amounts of data are not in the same shape or structure which could be suitable for any type of work. In natural language processing(NLP) and artificial neural network(ANN), data preprocessing is essential before training or testing to facilitate the process by removing required outliers and scaling the dataset.
</p>
<h1>Spelling Correction</h1>
  <h2>1. Introduction and Problem Definition</h2>
  <p>
    When working on non-quality-controlled, human-generated text data, lexical errors can be problematic for automatic analysis. Automatically correcting these errors is a non-trivial task, however. The task itself can be split into two sub-tasks - error detection (1) and error correction (2) - with their own sets of challenges.
  </p>
  <p>
    <strong>Error detection</strong> is the task of finding incorrect words in a given sentence. Errors can be categorized in two major categories: non-word errors and word errors. Non-word errors consitute errors that are not real words, e.g. when someone writes &quot;thre&quot; instead of &quot;three&quot;. Word errors, on the other hand, are errors that are correct words used in the incorrect context, e.g. when someone writes &quot;tree&quot; instead of &quot;three&quot;. Word error detection is even more complex to solve than non-word errors, which are difficult to automatically detect in their own right.
  </p>
  <p>
    <strong>Error correction</strong> is the task of finding the correct word for a given sentence, given a (non-)word error. Like error detection, this task can also be done with or without contextual information. An example for non-contextual error correction would be the spelling algorithm introduced by Peter Norvig [include source]. The downside of these solutions, however, becomes apparent in the example of the typo &quot;thre&quot;. Did the author mean to write &quot;there&quot; or did they mean to write &quot;three&quot;? This information can only be obtained through the context, i.e. the words surrounding the text error.
  </p>
  <p>
    TransSpell is an attempt to solve the problem of contextual text error correction. It is the successor to <a href="https://github.com/Broconuts/FastSpell/">FastSpell</a>, a different spelling correction approach that was developed for this Study Project; it was abandoned due to performance issues. It is, however, well-documented and offers some interesting insights into some more specific issues of spelling correction.
  </p>

  <h2>2. Approach</h2>
  <p>
    Bidirectional Encoder Representations from Transformers (BERT) models [reference] provide a workflow for using models that were pre-trained in a masking task on large amounts of data. Simply put, in masking tasks random tokens of a sequence are "masked" and the model is required to predict which token is most likely to occur in the position of the masked token. BERT models have yielded promising results in diverse NLP tasks by continuing training the massive pre-trained models on a smaller dataset for a downstream task; they can, however, still be used for masking tasks. We can therefore use these pre-trained models to generate candidates for a token in a sentence given its context. While this approach is computationally expensive, meaning that it does not scale very well, this project can be considered a proof of concept and we can dismiss the issue of computational efficiency for now.
  </p>

  <h3>2.1 Model</h3>
  <p>
    Since the release of the original paper and models in 2019, multiple derivates of the BERT architecture have been developed. Some aim to reduce computational demand at the cost of some performance, such as [DistilBERT] https://arxiv.org/abs/1910.01108) and [ALBERTA](https://arxiv.org/abs/1909.11942). Others go the other way, improving the performance of the riginal models but being more computationally expensive to use, such as [XLNet](https://arxiv.org/abs/1906.08237) and [RoBERTa](https://arxiv.org/abs/1907.11692).
  </p>
  <p>
    We have opted to use a distillated version of a RoBERTa model (i.e. a more lightweight model based on a more complex architecture) with 82M parameters. The model we are using is a pyTorch-based implementation through [HuggingFace's transformers library](https://huggingface.co/transformers/).
  </p>

  <h2>3. Error Correction</h2>
  <p>
    We will first describe error correction, as the mechanism is quite straightforward and will actually play a part in error detection as well. Given a word error and its surrounding sentence, we simply let the pre-trained RoBERTa model predict, which token should be in the given position based on its concept. Sentence length is an important factor in how well this approach will perform. For one-word answers, for example, it will not work at all. Furthermore, the pre-trained models seem to unproportionally weigh the positional context in the first and last token of a sequence, e.g. by suggesting replacement of the first token with tokens like bullet points or similar beginning-of-line indicators. Because of this, this error correction approach will only work properly on tokens that are preceded by at least one token and followed by at least one token.
  </p>
  <p>
    One could easily supply this model with an auxiliary candidate generation algorithm for these cases; as this is not necessary for the proof of concept and would increase the scope of this project even further, this auxiliary algorithm is omitted from this project.
  </p>

  <h2>4. Error Detection</h2>
  <p>
    In order to take full advantage of the context-sensitive error correction, our spelling corrector should be capable of detecting word errors (such as "tree" instead of "three"). This, in turn, requires our error detection to be context-sensitive as well. In this section, a concept for a context-sensitive approach will be introduced. FUrthermore, challenges that arise with this approach will be discussed, as well as the reasons for why we might have to rely on context-insensitive error detection for now.
  </p>
    <h3>4.1 Context-Sensitive</h3>
    <p>
      For context-sensitive error detection, we use the same technology as for error correction. Given a sentence consisting of n tokens, we create n substrings where one token is masked; for example, in substring 2, the second token of the sentence is masked. We then let the model determine 25 likely lexical candidates for the masked position. If the original token is not among the generated tokens, we consider the original token to be a text error. Due to the peculiar behavior regarding tokens in the first and last position of a sentence described in the previous section, the first and last token in a sentence need to be excluded from context-sensitive error detection.
    </p>
      <h4>The Issue</h4>
      <p>
        This approach comes with two major flaws, not to mention high computational costs, reducing the scalability of this approach. These will be described below.
      </p>
      <p>
        Firstly, by masking a random word and letting the model determine a likely candidate for that word, we assert that the context is correct. Let us assume a sentence with a word-error in the third token, e.g. "We made ensure that our customers get the products on time" when the author really meant "We made sure[...]". Error detection iterates through the sentence, starting with the second token, "made". Given the context, i.e. "we [mask] ensure that [...]", the model will not generate the correct "made" but rather deem the actual token erroneous and supply something along the lines of "must". The sentence now reads "we must ensure that [...]", which is now contextually coherent but not what the original author had in mind. If the contextual error does not occur in the first checked position (i.e. the second token) of a sentence, TransSpell will make the context fit the original error, rather than make the error fit the context.
      </p>
      <p>
        Secondly, this approach to error-detection can be boiled down to "making sure that a given sentence is a sequence of words that is statistically likely". This entails that, rather than "just" looking for errors, we remodel sentences according to what the pre-trained model has learned sentences to look like. For example, the sentence "We work hard to meet consumer requirements" is changed to "We work hard to meet minimum requirements". This is very problematic because it changes the meaning of the sentence.
      </p>
    <h3>4.2 Context-Insensitive</h3>
    <p>
      For the reasons described above, we cannot rely on context-sensitive error correction for now. Unfortunately, context-insensitive error detection solutions are error-prone as well. Usually, these approaches utilize finite lists of correct words, like lexica, and checking whether a token is contained in this finite list. As is the norm with assets like that, these lists are not exhaustive. While there are -potentially - exhaustive options out there (e.g. the Merriam-Webster API), these options are not for free and therefore not feasible for this project. We therefore introduce a list of additional conditions besides "membership in a lexicon", in an attempt to reduce the amount of False Positives in error detection.
    </p>
    <p>
      The complete process of context-insentive error detection for a given token is as follows (in order):
    </p>
    <ol>
        <li>Check whether the token is longer than three characters. If it is not, do not check further and assume it is not an error. This aims to protect acronyms and similar domain-specific terms from being considered an error.</li>
        <li>Check whether the token occurs more than 10 times in the entire corpus. If it does, do not consider it an error.</li>
        <li>Check whether the token is contained in a lexicon for American English and British English, respectively. If none of these conditions were met, consider the word an error. Please note that the second criterion requires knowledge of the entire corpus (as well as a certain size of the corpus for the measure to be effective).</li>
    </ol>
<h1>Named Entity Recognition</h1>
    <p>
    Named entities are discrete things, like Osnabrueck University, Siemens, Frank Sinatra, etc. Dates, sums of money, percentages and other numeric measurements are also named entities. Named Entity recognition (NER) is a task of finding and classifying named entities from a text into predefined categories to extract further information. Some of the most common use cases for NER are date recognition, allowing to put an event into a virtual calendar or linking on Wikipedia, establishing connections between certain words/word groups and related articles.
    </p>
    <h2>Implementing Named Entity Recognition</h2>
    <p>
    There are numerous ways to implement a NER pipeline. Throughout this study project, we tried to implement some of the most well-known ones to see which worked best. In doing so, we moved from the most simple to the more labor-intensive solutions.
    </p>
    <h3>NER with NLTK</h3>
    <p>
    The first way to implement NER we tried was the built-in NER function of the Natural Language Processing Toolkit (NLTK), a suite of NLP libraries for the English language.

    <p>
    NER in NLTK requires several pre-processing steps:
    </p>
    <p>
•	First, a text needs to be split into separate words or tokenized by applying nltk.word_tokenize(sentence). The result looks like: (This, is, a, sentence).
    </p>
•	Second, after applying nltk.pos_tag(sentence) each word receives its own part-of speech-tag (POS tag) The possible tags look like: we – PRP, dog - NN and so on.
    <p>
•	In the final step, the NER itself is implemented by chunking the preprocessed text with NLTK’s chunker: ne_tree = ne_chunk(pos_tag(word_tokenize(example))).
When performing NER, NLTK is able to determine such named entity categories as ORGANIZATION, PERSON, GPE, etc.
    </p>
    <p>
    However, NLTK’s named entity recognition abilities have certain limitations. When working with NLTK, the results we got not always matched our expectations and showed that some words were falsely determined as named entities, e.g. Overall – GPE; while others were assigned incorrect name categories, e.g. Microsoft – PERSON or AI – ORGANISATION. This in a way confirms the out of the box NER comparison results we were able to obtain online. There, NLTK is reported to be one of the fastest and easiest ways to perform NER tasks that is partially balanced off by its producing not the most accurate results in certain cases. When tested against other NER tools, NLTK’s F1 score matched those of Spacy and even beat those of Stanford NLP and Deep Pavlov pipelines. Though NLTK is essential for basic preprocessing tasks, it was decided to further explore the alternative NER methods as NLTK’s performance seemed to leave enough room for improvement.  Further options included Spacy and training a custom NER solution with our own dataset-specific tags from scratch. Please find both of these aspects covered in detail in the following sections.
    </p>
    <h4>References</h4>
    <p>
    Kalyan, P. (2019, August 09). Named Entity Recognition with NLTK. Retrieved September 08, 2020, from https://medium.com/@pavankalyanpk986/named-entity-recognition-with-nltk-ec59ff6a9250
    </p>
    <p>
    Natural Language Toolkit, Documentation. Retrieved September 09, 2020, from https://www.nltk.org/
    </p>
    <p>
    Li, S. (2018, December 06). Named Entity Recognition with NLTK and SpaCy. Retrieved September 08, 2020, from https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da
    </p>
    <p>
Terry-Jack, M. (2019, May 03). NLP: Pretrained Named Entity Recognition (NER). Retrieved September 08, 2020, from https://medium.com/@b.terryjack/nlp-pretrained-named-entity-recognition-7caa5cd28d7b
    </p>
  <h2>5. Evaluation</h2>

  <h2>6. Discussion</h2>

<h1>Named Entity Recognition</h1>
<p>
  Named Entity Recognition (NER) is a part of information extraction that detects and categorizes named entities in a raw text such as organization, person, location, date, monetary values, etc. NER can be typically used for Machine Translation, Semantic Annotation, and such tasks in Artificial Intelligence including Natural Language Processing.
</p>

  <h2>Part-of-Speech Tagging and Chunking with NLTK</h2>
  <p>
    Before proceeding with NER, we need to process part-of-speech (POS) tagging and chunking based on POS-tags. POS tagger is used to label grammar information corresponding to each word in a sentence using information like front and back words and shape features, such as upper and lower case, capitalization, etc. Chunking takes as input POS-tags, it extracts phrases composed of different tokens from the text. Especially, noun phrase (NP) chunking is a necessary process for NER because single word tokens may not fully represent the actual meaning of the text if a named entity consists of multiple words.
  </p>
  <p>
    As a first approach to POS tagging and chunking, Python’s NLTK library has been mainly used for data preparation in this project, also features a solid sentence tokenizer and POS tagger. First, NLTK tokenizes sentences based on words and punctuation, then the tagging is done by a trained model of the NLTK library. A chunk grammar can be defined using regular expression and works on top of POS-tags. As such, the performance was improved by modifying the chunk grammar based on the early result following POS-tagging, but NER using its default model still gave us inaccurate labeling.
  </p>

  <h2>Named Entity Recognition with spaCy</h2>
  <p>
    So, the next method with a pre-trained model for general entities like people, location, and date is Python’s spaCy library. SpaCy performs a highly efficient statistical processing for NER. It has not only similar features provided by NLTK such as tokenization, part-of-speech tagging, and named entity recognition, but also supports other functionalities, e.g. neural network models (multi-task CNN), integrated word vectors, and dependency parsing. But most of all, spaCy is optimized for production use, namely integrated and opinionated providing limited options selected based on functionality among other algorithms. This makes spaCy deliver better performance and usability with intuitive visualizations as well. Apart from its default model, spaCy also enables us to train and update the NER model with new examples, though it doesn’t support different neural network architectures.
  </p>
  <p>
    First, we chose spaCy’s small model without word vectors, which can be the right tool with fairly high accuracy (F-score: 0.85) and speed to get started with NER. The model was applied to our data, it worked relatively well for date and monetary entities, but most of the others, especially for company-specific words, were given a blank space or incorrect label. The large model might give slightly better performance (F-score: 0.86), but it’s not effective enough to offset the trade-off between coverage and memory usage that the increased size of the vector table brought. Additionally, the problem was that our dataset didn’t have sufficient named entities overall to train a custom model for noticeable improvement.
  </p>

  <h2>Discussion</h2>
  <p>
    Those results might be due to the fact that our data is unstructured texts collected from an employee survey which usually includes company-related words such as the name of its project, product, or system. So, if we tried to annotate it manually, we still had to get further information to set the criteria for what categories company-specific names belong to. This is one of the general challenges in the field of entity recognition that NER systems are often given sensitive data depending on the project. It means that even if a suitable model for one project is designed, it may not be reusable for another project. In conclusion, there is a need to develop a NER model that can eventually cover a wider range of data.
  </p>

  <h2>References</h2>
  <p>
      1. Bhavani, D. (n.d.). Understanding Named Entity Recognition Pre-Trained Models. Retrieved September 7, 2020, from https://blog.vsoftconsulting.com/blog/understanding-named-entity-recognition-pre-trained-models
      2. Jurafsky, D., & Martin, J. H. (2008). Speech and Language Processing, 2nd Edition (2nd ed.). Prentice Hall.
      3. Li, S. Named Entity Recognition with NLTK and SpaCy. (2018, August 17). Retrieved September 7, 2020, from https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da
      4. spaCy 101: Everything you need to know. (n.d.). Retrieved September 7, 2020, from https://spacy.io/usage/spacy-101
  </p>


<h1>Classification Model for NER detection </h1>
<h2>Introduction</h2>
<p>
Classification models are supervised learning models. Where the model learns from a pre-existing dataset. We have an input and a set output. The output are   finite or discrete variables making them a class. So, for every input the model learns to have an output class. We have a multi class output in the NER detection. For every given word input its output is a class output. The class output are categories of NER like person, location, organization.
</p>
<h2>Aim and Objective</h2>
<p>
The aim of creating a pipeline was to try to classify words to labeled entities. The objective were: </p>
<ul>
<li>To have a model to identify entities from the data provided to us. </li>
<li>To have a classification model with pre-defined labels. </li>
<li>A model functional for multiple classifications labels. </li>
</ul>
<h2>The Dataset</h2>
<p>
Taking an already existing GMB(Groningen Meaning Bank) corpus for entity classification which is pre annotated for NER. The dataset is tokenized at white spacing basically making it one-word token. Except for labels where multiple words are essential for an entity like organizations. Let’s take a list of entities in the training data with their corresponding description: </p>

<head>
<style>
table {
  font-family: arial, sans-serif;
  border-collapse: collapse;
  width: 100%;
}

td, th {
  border: 1px solid #dddddd;
  text-align: left;
  padding: 8px;
}

tr:nth-child(even) {
  background-color: #dddddd;
}
</style>
</head>

<h2>NER Labels</h2>

<table>
  <tr>
    <th>Labels</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>O</td>
    <td>All Tokens that are not an Entity</td>
  </tr>
  <tr>
    <td>geo</td>
    <td>Geographical Entity</td>
  </tr>
  <tr>
    <td>org</td>
    <td>Organization</td>
  </tr>
  <tr>
    <td>per</td>
    <td>Person</td>
  </tr>
  <tr>
    <td>gpe</td>
    <td>Geopolitical Entity</td>
  </tr>
  <tr>
    <td>tim</td>
    <td>time indicator</td>
  </tr>
  <tr>
    <td>art</td>
    <td>Artifact</td>
  </tr>
  <tr>
    <td>nat</td>
    <td>Natural Phenomenon</td>
  </tr>
  <tr>
    <td>eve</td>
    <td>Event</td>
  </tr>
</table>

<h2>The Packages </h2>
<p>The major two packages used for the classification model were Keras and Scikit-learn. Scikit learn is a machine learning library for the Python programming language. We used functions like count vectorizer, tf-idf and data splitting functions. Count vectorizer and tf-idf are basically used to convert the words to a numeric vector which can be fed to a neural network for processing. Data splitting function basically helps in splitting the data into a training, testing and blind dataset. Keras is a neural network package in Python. We use it to create a sequential dense model. “A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.” </p>

<h2>The Model </h2>
<ul>
<li>We create a basic sequential model of Recurrent Neural Network Layers. </li>
<li>We run all the Labels into the learning together. </li>
<li>The model predicts the probability of the word for each label trained. </li>
<li>The label with the highest probability is taken to be the classified label for the token. </li>
</ul>

<h2> Results and Output. </h2>
<p>
The following image shows the partial output accuracy of 3 classes number 0,1 and 2. Where 0 stands for geo, 1 for org and 2 for per. </p>
<img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/master/images/NERoutput1.png" alt="Accuracy" width="460" height="345">
<p>The second image shows how the output looks like and the corresponding pre-annotated correct labels </p>
<img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/master/images/NERoutput1.png" alt="Output" width="460" height="345">

<h2>Discussion</h2>
<p>Seeing the results and the code one can see that the classification model works better in terms of accuracy compared to the NLTK and the spacy model. However, we ran into a problem which we couldn’t solve. Once we used the pre existing annotated dataset we could test that data. But when we wanted to test the data on our dataset which is a blind dataset we weren’t able to run the model. The model is compatible to a certain input matrix. The blind data when vectorized using word vector or tf-idf has a different input matrix which is not compatible with the trained model. </p>

  <h2>Sources</h2>

<p> https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/data?select=ner_dataset.csv
https://keras.io/guides/sequential_model/ </p>


